<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="conjectures.html">Conjectures</a></div>
<div class="menu-item"><a href="CV_Aditya_Deshmukh.pdf">CV</a></div>
<div class="menu-item"><a href="misc.html">Miscellaneous</a></div>
</td>
<td id="layout-content">
<p>
</p>
<h1>Research</h1>
<p>
Following are high-level descriptions of reserach problems I have been working on.
</p>
<h2>Distributed and Adaptive Feature Compression</h2>
<p>
A prevalent way in which machine learning models are trained involves collecting data from various relevant sources, and training the models on the aggregated data. However, in many applications, the input data is often collected from distributed sources at inference time. Examples include, the Internet of Things (IoT) networks, security systems with surveillance sensors, and driverless cars collecting data from sensors and receiving data from wireless receivers. In these applications, the volume of data is generally high and decisions are time-sensitive, and so it is important to have low latency. Moreover, when the data is being communicated through wireless channels, bit-rates can be quite low  either for energy conservation purposes or because of poor channel conditions. Thus, it is imperative to optimize the data-stream pipelines in order to provide maximum information relevant to the performance of the downstream task. Moreover, in practice these pipelines are also subject to changes in bit-rates, and so it is necessary for the solutions to be adaptive to these changes. In this work, we try to answer the following question: <br />
<br />
<i>How to maximize information (relevant to the downstream task) received at a pretrained model at inference time when input data is collected in a distributed way through communication-constrained channels that are subject to change?</i> <br />
<br />
</p>
<center><table class="imgtable"><tr><td>
<img src="figs/distributed.png" alt="alttext" width="525px" height="225px" />&nbsp;</td><tr/>
<tr><td><center></center>
</td></tr></table></center>
<h2>Robust Estimation</h2>
<p>Robust mean estimation in high dimensions has received considerable interest recently, and has found applications in areas such as federated learning. The goal is to robustly estimate the true mean of a sample (drawn from an a distribution with unknown mean and bounded covariance matrix) which is corrupted by an adversary who has unlimited computational power, knows the estimator being used, and can corrupt upto \(\epsilon&lt;1/2\) fraction of the sample. We provided an optimal estimator which achieved the information-theoretic limit.
</p>
<table id="TABLENAME">
<tr class="r1"><td class="c1"><b>Algorithm</b> </td><td class="c2"> <b>Complexity</b> </td><td class="c3"> <b>Error guarantee</b> </td><td class="c4"> <b>Break-down point</b> </td><td class="c5"> <b>Needs \(\epsilon\)</b></td></tr>
<tr class="r2"><td class="c1">Tukey median </td><td class="c2"> NP-hard </td><td class="c3"> \(O(\sigma\sqrt{\epsilon})\) </td><td class="c4"> \(\frac{1}{d+1}\) </td><td class="c5"> No</td></tr>
<tr class="r3"><td class="c1">Diakonikolas et. al. 2017  </td><td class="c2"> \(\mathrm{poly(d,1/\epsilon)}\) </td><td class="c3"> \(O(\sigma\sqrt{\epsilon})\) </td><td class="c4"> NA </td><td class="c5"> Yes</td></tr>
<tr class="r4"><td class="c1">Cheng et. al. 2019  </td><td class="c2"> \(\tilde{O}\left(\frac{nd}{\epsilon^6}\right)\) </td><td class="c3"> \(O(\sigma\sqrt{\epsilon})\) </td><td class="c4"> \(\frac{1}{3}\) </td><td class="c5"> Yes</td></tr>
<tr class="r5"><td class="c1">Dong et. al. 2019 </td><td class="c2"> \(\tilde{O}(nd)\) </td><td class="c3"> \(O(\sigma\sqrt{\epsilon})\) </td><td class="c4"> NA </td><td class="c5"> Yes</td></tr>
<tr class="r6"><td class="c1">Zhu et. al. 2022 </td><td class="c2"> \(\tilde{O}(n^2d)\) </td><td class="c3"> \(O\left(\sigma\frac{\sqrt{\epsilon}}{1-2\epsilon}\right)\) </td><td class="c4"> \(\frac{1}{2}\) </td><td class="c5"> Yes</td></tr>
<tr class="r7"><td class="c1">Feasibility problem 2022 </td><td class="c2"> NA </td><td class="c3"> \(O\left(\sigma\sqrt{\frac{\epsilon}{1-2\epsilon}}\right)\) </td><td class="c4"> \(\frac{1}{2}\) </td><td class="c5"> Yes</td></tr>
<tr class="r8"><td class="c1">Opt. problems<b> 2022 </td><td class="c2"> NA </td><td class="c3"> \(O\para*{\sigma\sqrt{\frac{\epsilon}{1-2\epsilon}}}\) </td><td class="c4"> \(\frac{1}{2}\) </td><td class="c5"> No</td></tr>
<tr class="r9"><td class="c1">Algo 1</b> </td><td class="c2"> \(\tilde{O}(nd)\) </td><td class="c3"> \(O(\sigma\sqrt{\epsilon})\) </td><td class="c4"> \(1-\frac{1}{\sqrt{2}}\approx 0.3\) </td><td class="c5"> No</td></tr>
<tr class="r10"><td class="c1">Algo 2*  </td><td class="c2"> \(\tilde{O}(\max\{\epsilon n^2d, nd^2\})\)  </td><td class="c3"> \(O\left(\sigma\frac{\sqrt{\epsilon}}{1-2\epsilon}\right)\) </td><td class="c4"> \(\frac{1}{2}\) </td><td class="c5"> No
</td></tr></table>
</td>
</tr>
</table>
</body>
</html>
