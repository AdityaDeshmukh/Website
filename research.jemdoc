# jemdoc: menu{MENU}{research.html}

= Research

Following are high-level descriptions of reserach problems I have been working on.
== Distributed and Adaptive Feature Compression

A prevalent way in which machine learning models are trained involves collecting data from various relevant sources, and training the models on the aggregated data. However, in many applications, the input data is often collected from distributed sources at inference time. Examples include, the Internet of Things (IoT) networks, security systems with surveillance sensors, and driverless cars collecting data from sensors and receiving data from wireless receivers. In these applications, the volume of data is generally high and decisions are time-sensitive, and so it is important to have low latency. Moreover, when the data is being communicated through wireless channels, bit-rates can be quite low  either for energy conservation purposes or because of poor channel conditions. Thus, it is imperative to optimize the data-stream pipelines in order to provide maximum information relevant to the performance of the downstream task. Moreover, in practice these pipelines are also subject to changes in bit-rates, and so it is necessary for the solutions to be adaptive to these changes. In this work, we try to answer the following question: \n
\n
/How to maximize information (relevant to the downstream task) received at a pretrained model at inference time when input data is collected in a distributed way through communication-constrained channels that are subject to change?/ \n
\n
~~~
{}{img_center}{figs/distributed.png}{alttext}{525}{225}{}
~~~

== Robust Estimation
Robust mean estimation in high dimensions has received considerable interest recently, and has found applications in areas such as federated learning. 
~~~
{}{table}{TABLENAME}
Algorithm | Complexity | Error guarantee | BD point | Needs $\epsilon$||
Tukey median | NP-hard | $O(\sigma\sqrt{\epsilon})$ | $\frac{1}{d+1}$ | No||
Diakonikolas et. al. 2017  | $\mathrm{poly(d,1/\epsilon)}$ | $O(\sigma\sqrt{\epsilon})$ | NA | Yes||
Cheng et. al. 2019  | $\tilde{O}\left(\frac{nd}{\epsilon^6}\right)$ | $O(\sigma\sqrt{\epsilon})$ | $\frac{1}{3}$ | Yes||
Dong et. al. 2019 | $\tilde{O}(nd)$ | $O(\sigma\sqrt{\epsilon})$ | NA | Yes||
Zhu et. al. 2022 | $\tilde{O}(n^2d)$ | $O\left(\sigma\frac{\sqrt{\epsilon}}{1-2\epsilon}\right)$ | $\frac{1}{2}$ | Yes||
Feasibility problem 2022 | NA | $O\left(\sigma\sqrt{\frac{\epsilon}{1-2\epsilon}}\right)$ | $\frac{1}{2}$ | Yes||
Opt. problems* 2022 | NA | $O\para*{\sigma\sqrt{\frac{\epsilon}{1-2\epsilon}}}$ | $\frac{1}{2}$ | No||
Algo 1* | $\tilde{O}(nd)$ | $O(\sigma\sqrt{\epsilon})$ | $1-\frac{1}{\sqrt{2}}\approx 0.3$ | No||
Algo 2*  | $\tilde{O}(\max\{\epsilon n^2d, nd^2\})$  | $O\left(\sigma\frac{\sqrt{\epsilon}}{1-2\epsilon}\right)$ | $\frac{1}{2}$ | No
~~~